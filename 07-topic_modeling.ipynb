{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA - Latent Dirichlet Allocation is based on probability dristribution (Dirichlet distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr = pd.read_csv('../../pythongyak/UPDATED_NLP_COURSE/05-Topic-Modeling/npr.csv', sep='\\t')\n",
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nr of articles\n",
    "len(npr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nr of tokens is frist article\n",
    "len(npr['Article'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 # preprocessing: vectorisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ignore terms with too high doc frequency or too low (ratio 0-1 or an integer)\n",
    "# remove stopwords\n",
    "cv = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document term matrix = vectorised article column\n",
    "dtm = cv.fit_transform(npr['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 # perform LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# parameters: nr of components (~ topics), random state\n",
    "LDA = LatentDirichletAllocation(n_components=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit lda to our dt-matrix\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 # grab vocab of words\n",
    "\n",
    "# nr of terms\n",
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.get_feature_names()[30012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random word \n",
    "import random\n",
    "\n",
    "random_word_id = random.randint(0,54777)\n",
    "cv.get_feature_names()[random_word_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 # grab topics\n",
    "\n",
    "# nr of topics\n",
    "len(LDA.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA.components_ is a numpy array containing probabilities for each word\n",
    "type(LDA.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 # grab highest probability words per topic\n",
    "\n",
    "single_topic = LDA.components_[0]\n",
    "\n",
    "# returns sorted array with elements' original index position (see example below)\n",
    "# shows location of higher values in LDA.comp -- higher probabilities -- better candidates for the topic\n",
    "single_topic.argsort()\n",
    "\n",
    "# use the index positions (that are same as in cv.get_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "import numpy as np\n",
    "\n",
    "arr =  np.array([10, 200,1])\n",
    "arr.argsort()\n",
    "# returns array([2, 0, 1]) ---> [1, 10, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argsort returns index positions from least to greatest\n",
    "# we are looking for 10/20 greatest values --> grab last 10/20 values\n",
    "# that returns index position of ten most probable words\n",
    "top_ten_words = single_topic.argsort()[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words at index\n",
    "for index in top_ten_words:\n",
    "    print(cv.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 20 words of each topic\n",
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f\"Top 15 words for topic #{index}\")\n",
    "    print([cv.get_feature_names()[index] for index in topic.argsort()[-20:]])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach topic number to original articles\n",
    "dtm # original dt-matrix\n",
    "npr # original dataframe\n",
    "\n",
    "topic_results = LDA.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities of articles belonging to each topic\n",
    "# (one 7-dim row for each of the 119992 article)\n",
    "topic_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities of first article with actual percentages (rounded to 3 decimal)\n",
    "# probably belong to topic 2 ~ political topic\n",
    "topic_results[0].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can confirm by taking a look at the actual article\n",
    "npr['Article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns index position (topic nr) of highest probabilty\n",
    "topic_results[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add topic number column to original dataframe\n",
    "# (axis 1 --> 7 from topic_results.shape => (11992, 7))\n",
    "# ()\n",
    "npr['Topic'] = topic_results.argmax(axis=1)\n",
    "npr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-negative Matrix Factorisation:\n",
    "\n",
    "simultaneous dimensionality reduction and clustering\n",
    "\n",
    "the input is a non-negative data matrix, A (here: dtm).\n",
    "nr of basis vectors, k (number of topics)\n",
    "initialise W and H as random matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "npr = pd.read_csv('../../pythongyak/UPDATED_NLP_COURSE/05-Topic-Modeling/npr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 # construct vector space model for docs --> td-matrix\n",
    "# 2 #  apply tf-idf term weight normalisation on the td-matrix\n",
    "# 3 # normalise tf-idf vectors to unit length\n",
    "# 4 # initialise factors using non-negative double singular value decomposition (NNDSVD) on the td-matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # combines the steps of CountVectorizer and TfidfTransformer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = tfidf.fit_transform(npr['Article']) # call it dtm, although it is more than that as we used tfidf\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 # apply projected gradient nonneg matrix factorisation (NMF) on the td-matrix\n",
    "# --> basis vectors, W (topics/clusters) -- prob. of words belonging to a topic (each word is a row)\n",
    "# --> coefficient matrix: membership weights for documents relative to each cluster -- prop of docs belongign to a topic (each doc is a row)\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(n_components=7, random_state=42)\n",
    "nmf_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a specific token\n",
    "tfidf.get_feature_names()[2330]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show top 15 words per topic\n",
    "for index,topic in enumerate(nmf_model.components_):\n",
    "    print(f\"Top 15 words for topic #{index}\")\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach topic number to dataframe\n",
    "topic_results = nmf_model.transform(dtm)\n",
    "npr['Topic number'] = topic_results.argmax(axis=1)\n",
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach topic label to dataframe\n",
    "my_topic_dict = {0:'Health',1:'Campaign',2:'Legislation', 3:'Foreign politics', 4:'Election', 5:'Music', 6:'Education'}\n",
    "npr['Topic'] = npr['Topic number'].map(my_topic_dict)\n",
    "npr.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
